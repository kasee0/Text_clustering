{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home1/kasee0/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "import pprint\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./filtered_Main.pkl', 'rb') as f:\n",
    "    data1=pickle.load(f)\n",
    "with open('./filtered_Methods.pkl', 'rb') as f2:\n",
    "    data2=pickle.load(f2)\n",
    "with open('./filtered_Sum.pkl', 'rb') as f3:\n",
    "    data3=pickle.load(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='allenai/scibert_scivocab_uncased'\n",
    "\n",
    "auto_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "auto_model= AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The EDS spectra correlates with positions 1, 2, 3, 4, and 5, as shown in Figure 2c, which confirms the presence of Cr, Gd, Ti, O, and N elements in the respective layers\n"
     ]
    }
   ],
   "source": [
    "text_as_string=str(data1[0].split('.')[14])\n",
    "print(text_as_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_tokens = auto_tokenizer.tokenize(text_as_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'eds',\n",
       " 'spectra',\n",
       " 'correlates',\n",
       " 'with',\n",
       " 'positions',\n",
       " '1',\n",
       " ',',\n",
       " '2',\n",
       " ',',\n",
       " '3',\n",
       " ',',\n",
       " '4',\n",
       " ',',\n",
       " 'and',\n",
       " '5',\n",
       " ',',\n",
       " 'as',\n",
       " 'shown',\n",
       " 'in',\n",
       " 'figure',\n",
       " '2',\n",
       " '##c',\n",
       " ',',\n",
       " 'which',\n",
       " 'confirms',\n",
       " 'the',\n",
       " 'presence',\n",
       " 'of',\n",
       " 'cr',\n",
       " ',',\n",
       " 'gd',\n",
       " ',',\n",
       " 'ti',\n",
       " ',',\n",
       " 'o',\n",
       " ',',\n",
       " 'and',\n",
       " 'n',\n",
       " 'elements',\n",
       " 'in',\n",
       " 'the',\n",
       " 'respective',\n",
       " 'layers']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home1/kasee0/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin from cache at /home1/kasee0/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at allenai/scibert_scivocab_uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home1/kasee0/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home1/kasee0/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home1/kasee0/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home1/kasee0/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_embedding_model = models.Transformer('allenai/scibert_scivocab_uncased', max_seq_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence transformer를 사용해서 Word embedding model을 sentence embedding model로 만드는 방법\n",
    "\n",
    "pooling_model=models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "pooling_mode_max_tokens=False,\n",
    "Pooling_mode_mean_tokens=False,\n",
    "Pooling_mode_mean_sqrt_len_token=True)\n",
    "\n",
    "dense_model= models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(),out_features=768,activation_function=nn.Tanh())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model) #, dense_model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setence transformer를 사용하지 않고 word embedding model을 sentence embedding model로 변환하는 방법\n",
    "\n",
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "\n",
    "# mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# setences we want sentece embeddings for\n",
    "sentences = [\"I love you\", \"I hate you\", \"I love my dog\"]\n",
    "\n",
    "#Loda automodel from huggingface model repository\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "# Tokenize senteces\n",
    "encoded_input= tokenizer(sentences, padding=True, truncation = True, max_length=512, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "#Perform pooling.\n",
    "setence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence transformer를 사용해서 Word embedding model을 sentence embedding model로 만드는 방법\n",
    "# encoding\n",
    "\n",
    "%%time # 시간\n",
    "\n",
    "embeddings = model.encode(setntences, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% time\n",
    "\n",
    "umap_neighbors = 12 # local 값 숫자를 줄일 수록 local에 집중\n",
    "umap_n_components = 12\n",
    "\n",
    "umap_embeddings_12d = umap.UMAP(n_neigbors=umap_neighbors,\n",
    "                                n_componets=umap_n_components\n",
    "                                n_epochs=14000,\n",
    "                                min_dist=0.1,\n",
    "                                low_memory=False,\n",
    "                                learning_rate=0.5\n",
    "                                verbose=True,\n",
    "                                metric='cosine',\n",
    "                                spread=3.0,\n",
    "                                local_connectivity=2,\n",
    "                                target_metric='l2').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% time\n",
    "\n",
    "hdbscan_minimum_cluster_size = 55\n",
    "hdbscan_min_samples = 2\n",
    "\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=hdbscan_minimum_cluster_size,\n",
    "                          min_samples=hdbscan_min_samples,\n",
    "                          metric='euclidean',\n",
    "                          cluster_selection_epsilon=0.1,\n",
    "                          cluster_selection_method='leaf',\n",
    "                          leaf_size=40,\n",
    "                          algorithm='best').fit(umap_embeddings_12d)\n",
    "\n",
    "analyze_clusters= len(pd.Series(cluster.labels_).unique())\n",
    "\n",
    "df = pd.DataFrame(pd.DataFrame(cluster.labels_).value_counts())\n",
    "\n",
    "with pd.option_context('display.max_rows', 30, 'display.min_rows', 10, 'display,max_colums',10, 'display.width', 100):\n",
    "    print(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% time\n",
    "\n",
    "umap_neighbors = 12 # local 값 숫자를 줄일 수록 local에 집중\n",
    "umap_n_components = 3\n",
    "\n",
    "umap_data_3d = umap.UMAP(n_neigbors=umap_neighbors,\n",
    "                         n_componets=umap_n_components,\n",
    "                         n_epochs=14000,\n",
    "                         min_dist=0.1,\n",
    "                         low_memory=False,\n",
    "                         learning_rate=0.1\n",
    "                         verbose=True,\n",
    "                         metric='cosine',\n",
    "                         spread=3.0,\n",
    "                         local_connectivity=2,\n",
    "                         target_metric='l2').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3d = pd.DataFrame(umap_data_3d, colums=['x', 'y', 'z'])\n",
    "result3d['sentece'] = sentences\n",
    "result3d['labels'] = cluster.labels_\n",
    "\n",
    "with pd.option_context('display.max_rows', 30, 'display.min_rows', 10, 'display,max_colums',10, 'display.width', 200,'display.max_colwidth', 160):\n",
    "    print(result3d.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams and trigrams in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'fit_transfrom'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_74338/3050939135.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transfrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_per_topic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'fit_transfrom'"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,3), stop_words='english')\n",
    "count = vectorizer.fit_transfrom(docs_per_topic.Doc.values.tolist())\n",
    "\n",
    "t = count.toarray()\n",
    "w = t.sum(axis=1)\n",
    "\n",
    "tf = np.divide(t.T, w)\n",
    "sum_t = t.sum(axis=0)\n",
    "\n",
    "m=len(sentences)\n",
    "idf = np.log(np.divide(m,sum_t)).reshape(-1,1)\n",
    "\n",
    "tf_idf = np.multiply(tf,idf)\n",
    "tf_idf = np.array(tf_idf, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "labels = list(docs_per_topic.Topcic)\n",
    "\n",
    "tf_idf_transposed = tf_idf.T\n",
    "indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "\n",
    "tf = []\n",
    "top_n_words = {}\n",
    "\n",
    "def Sort_Tuple(tup):\n",
    "    tup.sort(key=lambda x: x[1])\n",
    "    return tup\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import result\n",
    "\n",
    "\n",
    "outliers3d = result3d.loc[result3d.labels == -1, :]\n",
    "clustered3d = result3d.loc[result3d.labels != -1, :]\n",
    "\n",
    "final_df = pd.merge(clustered3d, new_df2, left_on = ['labels'],right_on=['labels'])\n",
    "\n",
    "final_df_noise = pd.merge(outliers3d, new_df2, left_on = ['labels'],right_on=['labels'])\n",
    "\n",
    "final_df_sorted_labels = final_df.sort_values(by=['labels'], right_on = ['labels'])\n",
    "\n",
    "final_df_sorted_labels = final_df.sort_values(by=['labels'], ascending=True)\n",
    "\n",
    "my_final_df_sorted_labels_8 = final_df_sorted_labels.loc[final_df_sorted_labels['labels']==2]\n",
    "\n",
    "with pd.option_context('display.max_rows', 30, 'display.min_rows', 10, 'display,max_colums',10, 'display.width', 200,'display.max_colwidth', 160):\n",
    "    print(my_final_df_sorted_labels_8.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class my_ngrams:\n",
    "    def __init__(self,words_list, dim_n):\n",
    "        self.words_lsit = words_list\n",
    "        self.dim_n = dim_n\n",
    "\n",
    "    def generate_ngram(self):\n",
    "        ngrams = zip(*[self.words_list[i:] for i in range(self.dim_n)])\n",
    "        return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "    def ngrams_counter(self):\n",
    "        my_ngrams_counter = Counter(self.generate_ngram())\n",
    "        most_common_my_ngram = my_ngrams_counter.most_common()\n",
    "        most_common_my_ngram = pd.DataFrame(most_common_my_ngram)\n",
    "        most_common_my_ngram.colums = ['word','freq']\n",
    "        most_common_my_ngram['percentage'] = most_common_my_ngram['freq'] *100 / sum(most_common_my_ngram.freq)\n",
    "        return most_common_my_ngram\n",
    "\n",
    "for t in[0,1,2,3]:\n",
    "    my_words2 = final_df_sorted_labels.sentence.loc[final_df_sorted_labels['labels']==t].apply(lambda x: x.split()).tolist()\n",
    "    my_words_list2 = [item for sublist in my_words2 for item in sublist]\n",
    "    my_words_counter2 = Counter(my_words_list2)\n",
    "    gla = my_ngrams(my_words_list2, nn).ngrams_couter()\n",
    "\n",
    "my_words2 = final_df_sorted_labels.stentece.apply(lambda x: x.split()).tolist()\n",
    "my_words_list2 = [item for sublist in my_words2 for item in sublist]\n",
    "my_words_counter2 = Counter(my_words_list2)\n",
    "nn=3\n",
    "gla = my_ngrams(my_words_list2, nn).ngrams_counter()\n",
    "gla.head(12).style.format({'percentage': \"{:.3f}%\"}).bar(color='#FFA07A', vim=10, subset=['freq'], align='zero').highlight_max(axis=0,color='lightgreen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers\n",
    "pio.renderers.default = 'iframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(go.Scatter3d(x=final_df_nmoise.x,\n",
    "                             y=final_df_nmoise.y,\n",
    "                             z=final_df_nmoise.z,\n",
    "                             mode='text+markers',\n",
    "                             text=final_df_noise.labels,\n",
    "                             textfont=dict(size=9,family=\"Arial\",color ='rgba(128,128,128,0.0)'),\n",
    "                             hoverinfo='text',\n",
    "                             hoberterxt= final_df_noise.sentece,\n",
    "                             name = 'NOISE - no activ cluster allocation of these senteces',\n",
    "                             textposition='middle right',\n",
    "                             marker=dict(symbol='circle',\n",
    "                                         sizemin=13,\n",
    "                                         size=np.array(final_df_noise.magnitued),\n",
    "                                         sizeref=1400,\n",
    "                                         sizemode = 'diameter',\n",
    "                                         color='lightgray'),showlegend=True)\n",
    "                                         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('kasee0': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17d8a4c4a12e0fcc4da6a0d90074af0c88fc13083b0f38dd58621f1ae6738113"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
